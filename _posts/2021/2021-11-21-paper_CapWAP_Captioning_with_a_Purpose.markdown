---
layout: post
title: "paper : CapWAP: Captioning with a Purpose"
date: 2021-11-24 19:20:23 +0900
category: paper
---



#  CapWAP: Captioning with a Purpose

학회 - EMNLP 2020

소속 - csail.mit.edu

게재 - 9 Nov 2020

url - https://arxiv.org/pdf/2011.04264.pdf



# Abstract

기존 이미지 캡션 작업은 일반적인 참조 캡션을 사용해 이미지에 대한 텍스트 정보를 제공한다.

즉 이미지의 다른 시각적 관점에만 집중하게 된다 (어떤의미진지는 뒤에서 이어서 설명)

-> 의도된 인구의 정보 요구에 맞게 조정될 수 있는 시스템을 개발하는 것

이를 해결하기 위해 CAPWAP(Caption with a Purpose)를 제안



 질문 답변 모델이 샘플링된 사용자 질문에 대한 정답을 제공할 수 있는 출력을 보상함으로써 강화 학습을 사용하여 의도된 정보 요구에 맞게 직접 최적화할 수 있음을 보여준다.



# 1. Introduction

![f_1](E:\code\whtngus.github.io\img\2021\CapWAP_Captioning_with_a_Purpose\f_1.PNG)



일반 이미지 캡션의 경우 명확하지 않고 VQA는 질문에 대한 정보만을 제공함.

해당 논문에서는 

 (1) 일반 주석이 사용자의 정보 요구를 대표하지 않을 수 있고, 

(2) 사용자 질문이 정보 요구를 명확히 표현하는 더 자연스러운 방법이며, 

(3) 그러한 질문에 대한 정확한 답을 제공하기 위해 캡션을 최적화하면 훈련이 정보 요구에 집중할 수 있다고 주장

![t_1](E:\code\whtngus.github.io\img\2021\CapWAP_Captioning_with_a_Purpose\t_1.PNG)

1. 서로 다른 대상 사용자층이 표현하는 특정 정보 요구를 충족하기 위해 이미지 캡션을 생성하는 새로운 작업(CAPWAP)을 정의한다. 
2. 우리는 우리의 정보 중심 모델이 최첨단 기존 일반 캡션 시스템의 캡션보다 이 작업에 대해 훨씬 더 높은 품질의 캡션을 생성할 수 있음을 보여준다. 
3. 우리는 이 새로운 패러다임 하에서 강화 학습의 성능을 크게 향상시키는 새로운 합성 사전 훈련 루틴을 제안한다.

# 2. Related Work

사용자의 질문에 의해 표현된 정보 요구에 의존하여 보다 경험적인 접근법을 취한다.

빈번한 문구를 삭제하는 등 체계적인 사용적합성 문제로 어려움을 겪고 있다고 관찰했다

정보 품질을 평가하기 위해 QA를 사용하는 아이디어는 텍스트 요약을 위한 최근 연구에서 제안되었다

# 3. Problem Formulation

![f_2](E:\code\whtngus.github.io\img\2021\CapWAP_Captioning_with_a_Purpose\f_2.PNG)

- Task Setting

x : 임력 이미지

y : 캡션 

D : 샘플링된 질문-응답 쌍 (q-a)

- Information Need

QA 데이터가 아래와 같은 프로세스에 의해 도출된다고 가정

> 1. 이미지 x는 분포 p(x)에서 도출
> 2. D에서 사용자에게 중요한 것으로 인식되는 x의 정보 세부 사항을 대상으로 하는 (q,a)은 분포 p(q, a|x) 에서 도출됨
>
> (q,a) 쌍에 대한 한계 분포가 일반적인 사용자의 시각적 관심을 나타낸 것 

- Question Anticipation

완벽한 정답셋이 있다고 가정하지 않음

캡션 y는 잠재 변수이고 G(y|x)는 우리가 배워야 할 확률적 생성기라고 가정(완벽한 정답이 없기 때문에)

표본 y ~ G(y|x)는 무작위로 샘플링된 새로운 질문-응답에 대해 상황별로 제공해야 함 

논문에서는 y를 q에 대한 컨텍스트로 사용할 때 사전 훈련된 QA모델 M(q, y)의 정확도를 사용해 추정한다.

CAPWAP는 아래 기대치를 최대화 해야한다

![f1](E:\code\whtngus.github.io\img\2021\CapWAP_Captioning_with_a_Purpose\f1.PNG)

θ  : 파라미터 

R(y, q, a) : 리워드 

M(q, y) : output을 비교하기 위해 사용

- CapWAP vs. Other Tasks

표 1은 표준(일반) 캡션 및 시각적 질문 답변의 설정과 비교

VQA와 CAPWAP 모델은 QA 데이터로 교육 및 평가되지만, CAPWAP는 생성 전에 질문을 제공하지 않는다. (즉 질문 없이 이미지의 포인트인 답변을 캡션 하는것)

VQA 모델은 단일 답변을 출력하는 반면, CAPWAP 모델은 예상 컨텍스트를 출력 -> 모든 답변을 출력하기 위해 

# 4. An Approach to CAPWAP

훈련 중에 질문-답변 쌍에만 접근할 수 있고 추론 중에는 접근할 수 없다는 것을 고려할 때, 우리는 이 과제에 대한 모델을 어떻게 배울 수 있는가?

-> 이를 해경하기위해 강화학습을 사용

델이 생성된 각 캡션 y와 교육 QA 쌍(q, a)에 대해 보상 r(예: r = R(y, q, a)을 받는 강화 학습(RL) 프레임워크에 자연스럽게 부합

그러나 이러한 정책을 최적화하는 것은 모델이 처음에는 드문 사건인 정답(또는 부분적으로 정답)에 대해서만 보상을 받기 때문에 기술적인 문제를 제기



해당 논문에서는 아래와 같은 방법을 따름

1. 지도학습을 이용해  Gθ(y|x)를 학습해 파라미터를 생성 ( x^, y^) ~ D
2. Fine-tune G(y|x) QA data , (x, q, a) ∼ Dtarget.

D(generic)은 의도된 캡션 목적인 Dtarget에 대해 도메인 밖에 있는 것으로 가정

다양한 사용자 생성 질문과 정보 요구에 관심이 있기 때문에 일반적인 캡션 데이터는 종종 최종 목표와 크게 다를 수 있다. 



### 4.1 Model Architecture

이미지 캡션 시스템에서 공통되는 시퀀스 투 시퀀스 프레임워크를 따라 고속 R-CNN 및 트랜스포머 기반 인코더-디코더로 구성된 기본 캡션 모델을 간략하게 설명 (sota 모델 사용)

이미지 x가 주어지면, 우리는 먼저 사전 훈련된 고속 R-CNN 모델(Anderson 등, 2018)에서 계산된 탐지된 객체 경계 상자 임베딩 시퀀스로 나타낸다 -> 그다음 Transformer 기반 모델을 사용해  캡션 워드피스 y = (y1, . . . , yn)를 생성

### 4.2 Policy Training









