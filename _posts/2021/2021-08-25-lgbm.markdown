---
layout: post
title: "lgbm과 문제 case 공부"
date: 2021-08-25 19:20:23 +0900
category: deep_learning
---

# LGBM 

Light Gradient Boosting Machine의 약자인 **LightGBM** 은 원래 [Microsoft에서](https://en.wikipedia.org/wiki/Microsoft) 개발한 [머신 러닝을](https://en.wikipedia.org/wiki/Machine_learning) 위한 [무료 오픈 소스](https://en.wikipedia.org/wiki/Free_and_open_source) 분산 [그래디언트 부스팅](https://en.wikipedia.org/wiki/Gradient_boosting) 프레임워크



알고리즘은 GOSS(Gradient-Based One-Side Sampling) 및 EFB(Exclusive Feature Bundling)라는 두 가지 새로운 기술을 사용하여 높은 수준의 정확도를 유지하면서 알고리즘을 더 빠르게 실행



Light GBM은 Gradient Boosting 프레워크로 Tree 기반 학습 알고리즘

단, LGBM은 100개 이상의 파라미터를 가지고있기 때문에 파라미터 튜닝이 어려움

과적합에 민감하여 10,000개 이상의 데이터에 사용을 추천

## LGBM 학습 방식

![growth](.\img\2021\lgbm\growth.PNG)

출처 - https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/

기존 트리들은 tree depth를 줄이기 위해 균형 트리 분할을 사용했지만 lightgbm은 불균형 트리방식을 사용(빠른 대신 과적합에 민감해짐)

### 알고리즘

#### GOSS(Gradient-Based One-Side Sampling

- one-side sampling

데이터 갯수를 내부적으로 줄여서 계산할 때, 큰 gradient를 가진 데이터는 놔두고 작은 gradient는 랜덤하게 드랍

- one-side sampling 문제점 해결

상태에서 훈련하면 정확도가 낮아지게 되는데 따라서 낮은 gradient의 값들은 가져와서 버린 샘플만큼 뻥튀기한다. 1 – a / b 를 곱해서 수를 맞춰준다.

- 전체적인 과정 흐름도

> - 모델로 예측 및 loss 계산
> - loss로 정렬 후 상위 N개 추출하여 topSet에 저장 
>
> 이때 a 변수 사용 top 몇퍼센트를 가져갈지 예 30% 뽑는다면 0.3
>
> - topSet을 제외한 나머지 데이터 셋중 일부반 추출
>
> 이때 b 변수 사용 30% 추출시 b = 0.3
>
> - 위 두 단계에서 데이터 샘플링 완료 후 b로 추출된 데이터 셋의 weight를 1- a/b 취함
> - 위 단계를 거쳐 약한 예측기를 만들어 전체 예측기에 추가

#### EFB(Exclusive Feature Bundling)

- 아이디어 1

실제 데이터에서는 원핫인코딩과 같이 0이 굉장히 많은 희소행렬 형태인 경우가 많다.

-> 사실 트리모델에서 이렇게 상호 배타적인 (컬럼중 하나에만 값이 있고 나머지는 0인 경우) feature들은 하나로 통합이 가능하며, 이를통해 많은 계산량을 아낄 수 있다.

- 아이디어 2

feature가 몇가지만 값이 같다면 (수치형으로는 correlation이 높은) 몇가지 값들의 충돌을 무시하고 합쳐버리면 계산량을 좀더 줄일수 있다. (거의 비슷한 row data)

즉, data수(row) x feature수(column)에서 row X 합쳐진column(bundle) 이 되어 상당한 수의 계산이 줄어들 수 있다.





## LGBM 파라미터 (중요한 것만 정리)

| 파라미터              | 설명                                                         | default | 비고                                                         |
| --------------------- | ------------------------------------------------------------ | ------- | ------------------------------------------------------------ |
| max_depth             | Tree의 최대 깊이 - 과적합 및 모델 속도 등                    | -1      | -1은 깊이제한 없음                                           |
| num_leaves            | 하나의 트리가 가질 수 있는 최대 리프 개수                    | 31      |                                                              |
| min_child_samples     | 리프 노드가 되기 위해 최소한으로 필요한 레코드 수            | 20      |                                                              |
| early_stopping_rounds | 학습 조기종료를 위한 early stopping interval 값              | None    |                                                              |
| boosting_type         | 부스팅의 트리를 생성하는 알고리즘                            | gbdt    |                                                              |
| reg_alpha, reg_lambda | L1, L2 regularization                                        | 0.      |                                                              |
| learning_rate         | 학습 량                                                      |         |                                                              |
| bagging_fraction      | 데이터를 일부 발췌해서 다양성을 높이는 방법으로 사용 (민감한 옵션으로 feature_fraction과 잘 섞어서 사용 필요) |         | GOSS에서는 알아서 샘플링하는 과정이 있어서 에러 발생         |
| feature_fraction      | 열 샘플링, 컬럼에 대한 샘플링을 통해 각각의 높임             |         |                                                              |
| num_iterations        | 반복량                                                       |         |                                                              |
| scale_pos_weight      | 양성인 경우 데이터 뻥튀기, 불균형셋에서 유용할 수 있으나 너무 많은경우 정확도가 떨어짐 |         | 1.1~1.5 정도의 가벼운 수준 추천                              |
| boosting/booster      | 부스팅 방법 rf(random forest), gbdt(Gradient Boosted Decision Trees), dart(Drop out Regression Trees), goss(bradient-based one-side Sampling) | gbdt    | 정확도가 중요할때는 딥러닝 드랍아웃과 같은 dart 적용. 샘플링을 이용한 goss(속도 업, 정확도 내려감) 적용 |
| Metric/loss           | CE, MSE, MAE, MAPE, Log Transformation, Quantile 등 지원     |         |                                                              |













# 참고 사이트

https://en.wikipedia.org/wiki/LightGBM

https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html

https://ek-koh.github.io/data%20analysis/lgbm/

http://machinelearningkorea.com/2019/09/29/lightgbm-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0/

http://machinelearningkorea.com/2019/09/25/lightgbm%EC%9D%98-%ED%95%B5%EC%8B%AC%EC%9D%B4%ED%95%B4/