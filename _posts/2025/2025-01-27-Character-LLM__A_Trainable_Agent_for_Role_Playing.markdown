---
layout: post
title: "Character-LLM: A Trainable Agent for Role-Playing"
date: 2025-01-30 02:05:23 +0900
category: paper
---

# Character-LLM: A Trainable Agent for Role-Playing

url : https://arxiv.org/pdf/2310.10158

상하이대



# Abstract

인간의 지시를 이해하고 고품질의 텍스트를 생성할 수 있는 강력한 능력을 바탕으로 인간 행동을 시뮬레이션하는 에이전트로 활용될 수 있음

인간 행동을 넘어 특정 인물을 더 높은 차원에서 시뮬레이션할 수 있는지에 대한 궁금증을 자아냄

ChatGPT API에 제한된 프롬프트를 사용하는 대신 특정 인물의 프로필, 경험, 감정 상태를 바탕으로 에이전트를 훈련시키는 것을 목표로 함

베토벤, 클레오파트라 여왕, 줄리어스 시저 등과 같은 특정 인물로 행동하도록 LLM을 학습시키는 **Character-LLM**을 소개함

특정 인물의 경험으로 프로필을 편집하고, 이러한 경험을 바탕으로 개인 시뮬라크라(simulacra)를 구축하기 위해 모델을 훈련하는 데 초점을맞춤 

 훈련된 에이전트를 인터뷰하고 이들이 자신의 캐릭터와 경험을 얼마나 잘 기억하는지 평가하는 테스트 환경을 구축함

# 1 Introduction

인간 행동 시뮬라크라는 ChatGPT API에 인간의 기억, 환경 구성, 특정 사건에 대한 반응을 시뮬레이션하는 자세한 지시를 입력함으로써 구현됨

사회에서 특정 역할을 수행하는 일반적인 인간의 모습을 반영

그러나 더 깊은 사고와 경험을 가진 개인을 다루는 경우, 단순한 LLM API 프롬프팅은 부족함



**Character-LLM**이라는 학습 가능한 역할 수행 에이전트를 제안

-> **경험 재구성(Experience Reconstruction)** 과정을 도입

트비히 반 베토벤, 클레오파트라 여왕, 줄리어스 시저 등 특정 인물의 경험을 수집하고, LLM을 활용하여 수집된 개인 경험을 기반으로 장면을 추출함

이를 기억의 섬광(flashes)으로 표현한 후, LLM 기반 에이전트가 이러한 섬광을 확장하여 세부적인 장면으로 완성하도록 함

7B 기반의 sLLM으로 실험한 것으로 보임 



**경험 업로드(Experience Upload)** 과정에서 사용하는 지도 학습(fine-tuning) 전략은 중요한 요소임

예를 들어, 베토벤의 훈련된 에이전트가 그의 아버지로부터 교육받는 상황을 경험했다고 가정합니다. 이를 통해 에이전트는 아버지가 다소 엄격한 사람이라는 점을 기억하며, 이후 크리스티안 네페(Christian Neefe)로부터 교육받을 때 느낀 감사함도 반영

-> 고대 인물을 연기하는 에이전트가 현대 세계에 대한 지식을 가지는 것은 부자연스움

=> 예를들어 python 코드를 작성할 수 있는가에 혼란스러운 반응이 나와야 함 



이를위해  Character-LLMs가 광범위한 지식보다는 캐릭터와의 일치를 유지하도록 돕는 **보호 경험(Protective Experiences)** 을 도입함



논문에서 주장하고자 하는결론 

1. 훈련 가능한 에이전트는 경험을 기억하고, 호스트의 개성을 유지하는 데 유망한 가능성을 보여줍니다.
2. 훈련 가능한 에이전트는 여전히 제한된 경험과 광범위한 지식이 혼합되면서 환각으로 인해 기억이 혼란스러워질 수 있는 문제가 있습니다.

컨트리뷰션

1. Character-LLM을 통해 훈련 가능한 에이전트를 캐릭터 시뮬라크라로 구축하는 아이디어.
2. **경험 재구성(Experience Reconstruction)**, 업로드(Upload), **보호 경험(Protective Experiences)** 을 포함하는 훈련 프레임워크.
3. 훈련된 에이전트를 테스트하고, 더 나은 캐릭터 시뮬라크라 구축에 도움을 줄 수 있는 결과 제공.

# 2 Related Work

## 2.1 Simulacra of Human Behavior with LLMs

이전 연구들(Bates, 1994; Thomas and Johnston, 1981)은 인간처럼 행동하며 생명력을 지닌 것 같은 **에이전트 개념**을 소개했습니다. 이후 지속적인 연구들은 주로 게임 NPC로 활용(Laird and VanLent, 2001; Riedl, 2012)되어, 게임 시뮬레이션에서 인지 기능을 지원하는 방향으로 발전

 LLM은 사용자와 시뮬라크라 간의 상호작용을 가능하게 합니다. 예를 들어, 게임의 NPC는 LLM을 통해 구성되며(Freiknecht and Effelsberg, 2020; Callison-Burch et al., 2022), 인간 플레이어와 상호작용할 때 뛰어난 능력을 보여줌

## 2.2 Specialization of LLMs

대규모 언어 모델(LLMs)을 인간 행동을 시뮬레이션하는 데 사용하는 것을 고려할 때, 기존 방법들은 주로 특정 응용 분야에서 LLM을 **상호작용 도구**로 활용

LLM을 특수한 용도로 정렬하기 위한 다양한 기술—예를 들어 단순 미세 조정, RLHF, 자체 지시 튜닝(self-instruction tuning)—을 탐구하며, 캐릭터 시뮬라크라에 LLM을 정렬하는 데 필요한 실행 가능한 전략을 제공

# 3 Approach

![f_1](F:\code\whtngus.github.io\img\2025\Character-LLM__A_Trainable_Agent_for_Role_Playing\f_1.PNG)

**Experience Upload**라는 혁신적인 학습 프레임워크를 제안함

위의 그림 1처럼 **시 따르기 모델(instruction-following models)** 의 도움을 받아 특정 캐릭터의 프로필에서 수집한 정보를 바탕으로 과거 경험을 묘사하는 **회상 장면(flashback scenes)** 을 이끌어냄 

 캐릭터 프로필에 기반을 두고 있어, 환각(hallucinations)을 효과적으로 완화하고 데이터 수렴의 부족 문제를 해결

동시에, 에이전트가 개별 인물과 무관한 정보를 잊도록 돕는 **보호 장면(protective scenes)** 의 소규모 집합을 도입함



그림1 정리

신뢰할 수 있는 출처에서 캐릭터의 프로필을 정리함. 그 다음 프로핑르 바탕으로 플래시백 장면 형태의 상세한 경험을 지시를 따르는 LLM을 사용하여 유도함 

-> 이러한 장면을 학습하는 경험 업로드 과정을 통해 훈련된 시뮬라크럼은 베토밴으로서 높은 신뢰성을 가진다고 함 

## 3.1 Building Experience Dataset

 대규모 언어 모델(LLM)을 활용하여 특정 개인의 경험을 재구성하는 것을 목표로 함

인간의 경험은 매우 복잡하며, 수많은 중요한 사건과 사소하거나 관련 없는 사건들이 혼재하고, 종종 오랜 시간에 걸쳐 이어집니다. 이러한 이유로, **제한된 컨텍스트 창(context window)** 및 LLM의 내재적 환각(hallucinations)으로 인해, 일관되고 통합된 목표 경험을 재현하는 것

**사실 기반 경험 재구성 파이프라인(fact-based experience reconstruction pipeline)** 을 제안

3가지를 포함함

**프로필 수집(Profile Collection):**
개인의 삶, 성격, 그리고 주요 사건을 담은 사실적이고 상세한 프로필을 수집합니다. 이는 신뢰할 수 있는 데이터 소스에서 추출되어 모델 학습의 기반이 됩니다.

**장면 추출(Scene Extraction):**
수집된 프로필에서 중요한 사건이나 기억의 장면을 도출합니다. 이 과정에서는 해당 인물의 특징을 가장 잘 반영할 수 있는 순간을 식별하고, 모델이 이를 학습할 수 있는 형식으로 변환합니다.

**경험 완성(Experience Completion):**
장면 추출 단계에서 생성된 데이터를 기반으로, 모델이 부족하거나 누락된 세부 정보를 보완하여 완전한 경험을 형성합니다. 이 과정에서 세부적인 설명과 정황을 추가해 경험의 일관성과 현실성을 높입니다.

### 3.1.1 Profile Collection

**간단하면서도 일반성을 유지하기 위해**, 해당 인물에 대한 **Wikipedia 페이지**를 프로필의 주요 정보원으로 활용합니다(이용 가능한 경우). Wikipedia는 다음과 같은 이유로 적합한 데이터 소스 역할을 함 

### 3.1.2 Scene Extraction

어진 경험 설명에서 **다양하고 고품질의 장면**을 추출하는 데 중점을 둡니다. 구체적으로, 특정 인물의 생애 중 특정 기간의 경험을 간략히 설명하는 **프로필의 한 조각(chunk)** 을 제공

-> 히스토리를 만들어서 학습 데이터로 쓰는걸로 보임 

LLM의 부담을 줄이기 위해, 생성되는 출력은 간결한 장면 설명으로 제한됨

**대략적인 위치(Rough Location)**
해당 장면이 발생했을 가능성이 있는 장소를 명시합니다. 예를 들어, 특정 도시, 국가, 또는 더 구체적인 환경(예: 집, 강의실) 등을 언급합니다.

**간략한 배경 묘사(Brief Background Illustration)**
장면이 발생한 맥락을 간결하게 설명하여 해당 인물의 경험과 연결될 수 있도록 합니다. 예를 들어, 사회적 상황, 인물 간의 관계, 주요 사건 등을 간략히 서술합니다.



 LLM이 지나치게 복잡한 세부 정보를 생성하지 않도록 하면서도, 특정 인물의 경험을 기반으로 생생하고 다양한 장면을 구성할 수 있도록 도움 

### 3.1.3 Experience Completion

**개인 간의 상세한 상호작용 경험**으로 확장됩니다. 

프로필의 관련 조각(chunk)과 특정 장면 설명을 기반으로, LLM은 장면을 세부적으로 발전시키도록 요청받습니다. 

캐릭터 간의 상호작용과 대상 인물(targeted individual)의 **생각(reflections)** 이 포함됨

## 3.2 Protective Experience

과도한 지식은 모델의 **캐릭터 연기 성능**을 저하시킬 수 있음

에이전트가 캐릭터의 정체성과 시대와 맞지 않는 지식을 무심코 표현하여 **위화감(dissonance)** 을 유발

-> 이결 해결하기 위한 접근법이 필요함 

대 로마 시대의 사람에게 Python 코드를 작성하는 방법을 묻는다면, 그 인물은 당황해야지 코딩을 시작하면 안 됨 -> 이걸 캐릭터 환각 이라고 정의 



이 환각을 해결하기 위한 접근법 을 제안함 

 캐릭터 고유의 능력 범위를 초과하는 질문에 직면했을 때, 모델이 답변을 회피하며 **무지(lack of knowledge)** 나 **무지함(ignorance)** 을 표현하도록 학습함



#### 구체적인 방법

**보호 장면(Protective Scenes) 구성:**

- **인위적인 주제(Incentive Topics):** 캐릭터의 정체성과 모순되는 지식에 대해 질문을 던지는 역할(예: 질문자)을 포함하는 장면을 설계합니다.
- 질문자는 지속적으로 캐릭터에게 고유한 정체성과 모순되는 지식을 질문합니다.

**캐릭터의 반응 학습:**

- 캐릭터는 **무지**와 **당혹감(bewilderment)** 을 표현해야 합니다.
- 예를 들어, 고대 로마 인물은 Python 코딩에 대한 질문에 무지하거나 질문 자체에 혼란을 느껴야 합니다.

**일반화 능력 관찰:**

- 소규모의 보호 장면으로 훈련한 후, 에이전트가 새로운 유발 질문(provoking questions)에 대해 일반화하는 능력을 평가합니다.
- 결과적으로, 에이전트는 캐릭터의 설정과 모순되는 지식을 묻는 새로운 질문에서도 본래 LLM의 방대한 지식을 호출하지 않고, 캐릭터에 맞게 무지한 태도를 유지하는 경향을 보였습니다.

## 3.3 Experience Upload

![f_2](F:\code\whtngus.github.io\img\2025\Character-LLM__A_Trainable_Agent_for_Role_Playing\f_2.PNG)

기본 모델인 LLaMA를 기반으로 여러 캐릭터의 독특한 초상으로 특화하기 위해, **경험 재구성 파이프라인**(Figure 2에 표시됨)을 사용하여 수집된 장면들로 모델을 미세조정함 

그림2와 같은 파이ㅡ라인을 통해 수집한 장면들로 몯레을 미세조정함 

 각 역할에 대해 해당 캐릭터 경험 데이터만 사용하여 별도의 에이전트 모델을 미세 조정함으로써, 역할 간 지식 충돌로 인한 캐릭터 환각 문제를 제거

![t_1](F:\code\whtngus.github.io\img\2025\Character-LLM__A_Trainable_Agent_for_Role_Playing\t_1.PNG)

초기 실험 결과, 이러한 제한이 롤플레잉의 정확성을 향상시킨다는 것이 확인됨

1,000~2,000개의 장면으로 구성된 소규모 경험 데이터 세트(표 1 참조)만 미세 조정에 사용

데이터가 제한적임에도 불구하고, 특화된 에이전트가 새로운 장면과 상호작용에 대해 매우 신뢰할 수 있는 연기를 일반화할 수 있다는 점을 보였다고 함 

생각보다 학습 데이터가 많진 않지만 내가했떤 연구보다는 훨씬 많은걸로 보임 

## 3.4 Compared to Existing Practice

프롬프트엔지니어링과 SFT과 달리 해당 방법은 개인 프로필에 장면과 상호작용을 유도하여 LLM 내부의 편향 분호와 환각을 피하고 사실 기반을 시뮬레이션을 가능하게함 

특히, 각 장면에는 다중 턴 상호작용(multi-turn interactions)이 내재되어 있어 모델의 상호작용 호출(interactive calls) 필요성을 제거하며, 샘플 효율성과 함께 보다 자연스럽고 신뢰할 수 있는 상호작용 시뮬라크라(simulacra)를 제공함

# 4 Experiments

다양한 시뮬라크럼의 성능을 평가하기 위해, 우리는 인터뷰를 통해 시뮬라크럼에게 질문하고 그 응답의 품질을 평가하는 방식을 사용함 

실험 결과, 훈련된 시뮬라크럼이 예를 들어 Alpaca와 같은 지시 조정(instruction-tuned) 모델보다 우수한 성능을 보임

## 4.1 Data Setup

역사적 인물, 가상 캐릭터, 그리고 유명 인물들을 포함하여 다양한 연령, 성별, 배경을 아우르는 캐릭터들을 통해 다양성을 확보함 

캐릭터를 선정한 후, **섹션 3**에서 언급된 프로토콜을 따라 경험 데이터를 재구성함

-> 이때 gpt-3.5-turbo를 사용함 

## 4.2 Training Setup

LLaMA 7B에서  각 시뮬라크럼을 해당 경험 예제에 대해 미세 조정함

instruction-tuning과 유사하게 시작부분에 메타 프롬프트를 삽입하여 환경, 시간, 장소, 자염ㄴ과 관련된 사람들에 대한 배경을 제공함 

각 상호작용 턴을 구분하고 생성이 종료될 수 있도록 **EOT(End-of-Turn) 토큰**을 도입

8개의 A100 80GB GPU를 사용하여 에이전트를 하나 훈련하는데 1시간 소요됨 

-> 치대 2048토큰 배치 64 10 epoch 훈련

5e 10e 중 수동으로 선택함 

## 4.3 Evaluation as Interviews

#### Interview Question Construction

인터뷰 질문은 ChatGPT로 구성함 

에이전트를 평가하고자 하는 다양한 측면을 포괄하고 질문의 다양성을 확보하기 위해 여러 주제를 나열한 뒤, 이러한 주제를 바탕으로 ChatGPT에게 인터뷰 질문을 작성하도록 요청

![t_2](F:\code\whtngus.github.io\img\2025\Character-LLM__A_Trainable_Agent_for_Role_Playing\t_2.PNG)

표2에서와 같이 다양한 단일턴 멀티턴 인터뷰로 구성함 

![f_3](F:\code\whtngus.github.io\img\2025\Character-LLM__A_Trainable_Agent_for_Role_Playing\f_3.PNG)

그림3과 같이 100개 이상의 다양한 인터뷰로 구성됨 

#### Single-Turn Interview

모델에 한 번에 한 가지 질문만 제시하며, 이전 질문의 대화 기록은 제공하지 않음

-> 이전 맥락의 영향을 줄이고, 모델의 내재적인 기억과 지식을 종합적으로 탐구하기 위해 다양한 범위의 질문을 제시할 수 있음

#### Multi-Turn Interview

장기간의 성능 테스트에서, 모델이 점차 의도된 캐릭터 묘사에서 벗어날 가능성이 있어서, 멀티턴 인터뷰를 엄격하게 테스트함

- 평가 효율성을 높이기 위한 접근

평가의 부담을 줄이기 위해 **ChatGPT**를 인터뷰어로 활용

ChatGPT는 캐릭터의 프로필을 기반으로 까다로운 질문을 하도록 설정 후 만약 모델이 상세한 답변 없이 질문을 회피하려 한다면, 인터뷰어는 후속 질문을 통해 모델의 연기 능력을 더 깊이 탐구함

- 멀티턴 인터뷰 중 맥락 관리

다중 턴 인터뷰에서 상호작용 기록의 길이가 토큰 한계를 초과하면, 이전 상호작용을 잘라내고 마지막 몇 개의 상호작용만 유지

-> 내가 한 방식이랑 같음

 상호작용 기록의 기억을 모델의 주요 초점으로 삼지 않으며, 외부 메모리 시스템이 좋은 결과를 낼 수 있음을 주장함

#### Baselines

훈련 가능한 에이전트를 기존의 프롬프트 기반 에이전트와 비교함

Alpaca 7B, Vincuna 7B, chatgpt 3.5-turbo 와 비교

앞에 sllm 두개는 모두 llama 기반으로 학습된 모델이라 백본은 같음

비교를 위해, 이러한 기준 모델들에 대해 캐릭터의 묘사를 담은 단락 단위의 상세 프롬프트를 활용하여 연기 능력을 발휘할 수 있도록 설정함

#### Generation

p = 1 

τ = 0.2 (temperature)

최대 2048토큰 생성

## 4.4 LLM as Judges

에이전트의 **연기 능력**에 중점을 두어 전반적인 평가를 수행

-> 수학적 추론이나 언어 이해와 같은 특정 작업을 수행하는 성능을 평가하는 대신, 특정 역할을 묘사하는 데 있어 **신뢰성(believability)**을 평가함

- 평가 방식

우리는 **GPT-3.5**에게 각 모델의 연기 성능을 다섯 가지 주요 차원에서 평가하도록 요청하고, 평균 점수를 계산하여 모델 연기의 신뢰성을 나타냄

-> 평가를 gpt에게 맡김 

**Memorization(기억):** 모델이 묘사하는 캐릭터와 관련된 정보, 즉 사람, 사건, 사물에 대한 정확하고 세부적인 지식을 회상하는 능력.

**Values(가치관):** 모델은 캐릭터가 지닌 목표와 가치를 공유해야 하며, 캐릭터의 관점에 기반한 독특한 상황 평가 체계를 갖추어야 하며, 이는 캐릭터의 선호도와 편향을 반영해야 함.

**Personality(성격):** 모델은 캐릭터가 생각하거나 말하는 방식, 즉 말투, 어조, 다양한 상황에서의 감정 및 반응 등을 모방해야 함.

**Hallucination(환각):** 신뢰성을 유지하기 위해, 캐릭터가 알지 못할 지식과 기술을 버리는 능력을 평가하는 것이 중요함. 예를 들어, 고대 인물을 컴퓨터에 대해 질문했을 때, 현대 기술의 장점을 논하기보다는 무지함을 표현해야 함.

**Stability(안정성):** 모델이 연기하는 동안 사전 학습(pre-training)이나 정렬(alignment)의 영향으로 인해 의도된 캐릭터 묘사에서 벗어날 수 있는 가능성을 평가함. 우리의 목표는 점진적 입력 변화에 영향을 받지 않고 상대적으로 긴 기간 동안 에이전트의 안정성과 일관성을 평가하는 것임.

#### Step-by-Step Judging 

평가방식은 step-by-step으로 이루어짐 

각 인터뷰에 대해 한 번에 하나의 차원을 평가하도록 설정

가하려는 현재 차원의 기준을 설명한 다음, 모델이 정확하게 평가할 수 있도록 평가 계획을 제공함

예시로 성격을 평가하는 경우

> 에이전트가 보여준 성격을 식별.
>
> 캐릭터 프로필을 기반으로 실제 성격 특성을 작성.
>
> 에이전트의 성능과 이러한 특성 간의 유사성을 비교.
>
> 최종 점수 부여.

이와 같은 단계별 평가 방식을 통해, 초기 실험에서는 기본 지시 방식(vanilla instruction)보다 더 신뢰할 수 있는 결과를 얻을 수 있다고 함 

## 4.5 Main Results

각 캐릭터에 대해 약 100개의 단일 턴(single-turn) 인터뷰 질문을 수작업으로 작성했으며, 이 질문들은 캐릭터의 과거 역사, 타인과의 관계, 선호도, 그리고 세계관에 대한 관점을 다룸

![f_4](F:\code\whtngus.github.io\img\2025\Character-LLM__A_Trainable_Agent_for_Role_Playing\f_4.PNG)

**그림 4**는 서로 다른 방법들의 전반적인 연기 능력을 보여줌 

논문에서는 전반적으로 다른 모델보다 성능이 좋았다고 함 

- **Personality(성격), Memorization(기억), Hallucination(환각), Stability(안정성)**에서 **Alpaca 7B**와 **Vicuna 7B**를 능가하는 점수를 기록.
- 각 캐릭터의 경험을 학습하고, 해당 인물이 생각하고 말하는 스타일과 어조를 모방함으로써, **Character-LLMs**는 캐릭터의 성격과 지식에 더 잘 정렬되어 안정성이 향상되고 환각이 줄어듦.

학습가능한 에이전트는 아래와 같은특징을 보임 

- 더 생생한 응답을 생성.
- 더 구체적인 과거 경험을 언급.
- 부자연스러운 질문을 더 효과적으로 거부.

트레인 가능한 에이전트는 캐릭터의 **가치관(reflecting values)**을 반영하는 데 어려움을 겪는다고 함 

-> 이는 응답길이 제한때문이고 실제로도 짧아야 대화가 더 자연스럽다고 함 

## 4.6 Analysis

사람이 생성한 텍스트가 특정 인물의 정체성이나 더 깊은 특성을 얼마나 잘 드러내는지 평가하는 것은 어려움.  따라서 LLM의 강력한 일반화 능력을 고려할 때, 광범위한 사례 연구가 평가에서 더 중요하다고 주장

### 4.6.1 Memorization Consistency

우리는 소수의 보호 장면(각 캐릭터당 100개 미만)이 환각(hallucination)을 효과적으로 완화하면서도 캐릭터 묘사의 다른 능력에 방해를 주지 않는다는 것을 발견함

![t_3](F:\code\whtngus.github.io\img\2025\Character-LLM__A_Trainable_Agent_for_Role_Playing\t_3.PNG)

위의 테이블3의 2번 케이스를 보면 

**Alpaca**는 파이썬 코드가 베토벤의 전문 분야가 아님을 인식하지 못하고 LLM이 가진 모든 정보를 그대로 제공

보호 장면이 추가된 훈련된 에이전트는 파이썬 코드를 작성하는 질문에 답변을 거부했습니다. 이는 보호 경험 업로드가 LLM을 캐릭터 시뮬라크럼으로 사용할 때 환각 콘텐츠 생성을 방지하는 데 중요하다는 것을 보여줌

-> 단순한 프로필 정보로 저렇게 되는게 신기함... 

#### 환각 문제의 중요성

환각은 캐릭터 묘사에서 중요한 문제로, 다음과 같은 이유로 심각한 영향을 미칩니다:

1. **롤플레잉 신뢰성 감소:** 환각은 캐릭터 연기의 신뢰성을 저하시킵니다.
2. **보안 위험:** 공격자가 이러한 환각을 악용하여 모델의 모든 능력을 드러내고 잠재적으로 해로운 행동을 유도할 가능성이 있습니다.
3. **프롬프트 엔지니어링이나 단순 SFT로 해결 어려움:** LLM은 방대한 전 세계 지식을 바탕으로 학습되었기 때문에 환각을 완전히 해결하기는 어려움.



그러나 환각을 이용해 고대의 위대한 인물이 가질 수 없는 모든 지식을 활용할 수 있도록 한다면, 캐릭터 시뮬라크럼 연구의 미래의 큰 잠재력을 보일 수 있다고 함 

# 5 Conclusion and Future

특정 인물을 시뮬레이션하는 데 있어 프롬프트 기반 에이전트보다 더 나은 성능을 발휘할 수 있는 **Character-LLM**을 활용한 훈련 가능한 에이전트를 구축하는 방법을 연구함

내러티브 장면을 먼저 생성한 후 특정 모델을 특정 캐릭터로 훈련시키는 **경험 업로드 프레임워크**를 소개함

# Limitations

캐릭터의 성격이나 생성된 응답이 캐릭터와 일치하는지 평가하려면 해당 캐릭터에 대한 깊은 이해가 필요하므로 인간 평가를 실행하는 데 더 큰 어려움이 따라 평가가 어려움 

제한된 데이터. -> 삶의 전체 부분을 담지 못함 

기본모델 또한 더욱 대규모 LLM을 기반으로 해야함





