---
layout: post
title: "User Behavior Simulation with Large Language Model based Agents"
date: 2025-02-01 02:05:23 +0900
category: paper
---



# User Behavior Simulation with Large Language Model based Agents



2023년 1월 5일 

중국쪽 대학교, 런던대

url : https://arxiv.org/abs/2306.02552



# Abstract

사용자 행동 데이터를 시뮬레이션하는 것은 사람 중심 애플리케이션에서 항상 근본적인 문제로 여겨져 왔으며, 주요 어려움은 인간 의사결정 과정의 복잡한 메커니즘에서 비롯됨

LLM 기반 에이전트 프레임워크를 제안하고, 실제 사용자 행동을 시뮬레이션하기 위한 샌드박스 환경을 설계함



광범위한 실험을 바탕으로, 우리의 방법으로 시뮬레이션된 행동이 실제 인간의 행동과 매우 유사하다는 것을 발견

아래 두 가지 사회적 현상을 시뮬레이션하고 연구

1. information cocoons(정보 코쿤)
2. user conformity behaviors. (사용자 동조 행동)

# Introduction

사람을 중심으로 하는 AI 애플리케이션은 추천 시스템, 소셜 네트워크등이 있으며 이는 충분하고 신뢰할 수 있는 데이터 가용성이 필요하나 많은 비용이드는 딜레마가 있음 

이를 위해 사용자 행동 시뮬레이션 전략들이 제안됨 



기존 시뮬레이션들은 좋은 성과를 거두었지면 몇가지 한계가 있음 

1. 단순화된 사용자의 의사 결정 

   사용자의 의사결정을 시뮬레이션함.

   이는 인간의 인지 과정의 복잡한 멭커니즘과는 거리가멀며 사용자 행동을 생성하지 못할 가능성이 높음 

2. 실제 데이터 의존성

   데이터 세트를 수집할 수 없는 상황에서 사용자 행동을 시뮬레이션하는 것

   데이터를 먼저 구해서 연구에 사용해야되지만 데이터를 구하기 어려운 문제가 다시 발생함

3. 단순화된 시뮬레이션 환경

   기존 연구는 단일 시뮬레이션 환경에 그침

   그러나 실제 사용자 행동은 다양한 환경에서 발생하며, 이들은 서로 상호 영향을 미칠 수 있음



논문의 목적은 사용자 행동 시뮬레이션에 LLM을 활용할 가능성을 탐구하는 것

이를 위해 인지 신경과학에서 영감을 받아 프로파일 모듈(Profile Module), 메모리 모듈(Memory Module), 액션 모듈(Action Module)로 구성된 LLM 기반 에이전트 프레임워크를 설계

에이전트의 상호작용 행동을 관찰하기 위해, 에이전트가 웹 추천 시스템과 상호작용하고, 서로 대화하며, 친구들에게 정보를 방송할 수 있는 개입 가능하고 리셋 가능한 샌드박스 환경을 설계

이를 활용하여 정보 코쿤(Information Cocoons)과 사용자 동조(User Conformity)라는 두 가지 잘 알려진 사회적 현상을 연구하여 시뮬레이터가 이러한 현상을 잘 재현할 수 있음을 발견했으며, 실험 플랫폼으로서 우리의 시뮬레이터를 사용하여 이러한 현상을 완화할 수 있는 여러 잠재적 전략을 발견



# Methods

## The Agent Framework

**프로파일링 모듈**, **메모리 모듈**, **액션 모듈**을 포함한 에이전트 프레임워크를 설계하여 기존 LLM을 강화



### Profiling Module

사용자 프로파일은 추천 시스템에서 중요한 개념으로, 사용자의 **선호도**, **성격**, **행동 패턴**을 결정합니다. 시뮬레이터에서 사용자 프로파일은 다음 정보를 포함합니다:

- **기본 정보**: ID, 이름, 성별, 나이.
- **특성**: 사용자의 성격(예: "동정심이 많은", "야망 있는", "낙천적인").
- **관심사**: 항목에 대한 선호도(예: "SF 영화", "코미디 비디오").
- **행동 특징**: 사용자 행동 패턴을 구체적으로 기술.

ChatGPT를 활용해 추천 도메인에서 주목할 만한 **5가지 행동 특징**을 도출하여 사용자 프로파일에 포함함

**Watcher**:

- 상호작용한 항목에 대해 적극적으로 피드백과 평점을 제공합니다.

**Explorer**:

- 이전에 들어본 항목을 적극적으로 검색하며, 세부적인 경험을 제공합니다.

**Critic**:

- 항목에 대해 높은 기준을 요구하며, 추천 시스템과 항목 모두를 비판할 수 있습니다.

**Chatter**:

- 항상 친구와의 대화에 참여하며, 친구의 추천을 신뢰합니다.

**Poster**:

- 소셜 미디어에 공개적으로 게시하는 것을 즐기며, 친구들과 콘텐츠와 통찰을 공유합니다.

->  이 데이터는 GPT로 만들었으니 ... 의미가 있나 싶음 

### **프로파일 생성 전략**

시뮬레이터에서 유연하고 효율적으로 사용자 프로파일을 생성하기 위해 **3가지 프로파일 생성 전략**을 설계했습니다:

1. **수작업 생성 방식 (Handcrafting method)**
   - 에이전트의 프로파일을 수작업으로 지정합니다.
   - 예: "David Smith, a 25-year-old male photographer".
   - 매우 유연하며, 다양한 유형의 사용자를 쉽게 시뮬레이션할 수 있습니다.
2. **GPT 기반 생성 방식 (GPT-based method)**
   - GPT를 활용해 프로파일 정보를 생성합니다.
   - 적절한 프롬프트를 설계하여 자동으로 에이전트 프로파일을 생성합니다.
   - 예: "Here is the user’s profile table. Please refer to the existing information and continue to complete the user profile."
   - **효율성이 높아** 시간 비용을 대폭 줄일 수 있습니다.
3. **실제 데이터 정렬 방식 (Real-data alignment method)**
   - 실제 데이터셋의 사용자 정보를 활용해 프로파일을 생성합니다.
   - 예: 데이터셋에서 사용자가 상호작용한 항목을 에이전트 프로파일로 사용.
   - 실제 데이터를 시뮬레이션 과정에 통합할 수 있어, **현실적인 분포**(예: 성별 비율)를 반영합니다.

-> 데이터를 직접 만들어서 사용함  (역시 데이터는 없음듯 )

### Memory Module

인지신경과학 연구에 따르면, 인간 기억은 다음과 같은 세 가지 주요 부분으로 구성됩니다:

- **감각 기억 (Sensory Memory)**:
  환경에서 직접 인지된 정보를 처리하며, 정보는 몇백 밀리초 동안만 유지됩니다. 중요한 정보는 단기 기억으로 전달되고, 나머지는 폐기됩니다.
- **단기 기억 (Short-term Memory)**:
  감각 기억에서 전달된 정보를 처리합니다. 반복 경험으로 인해 강화된 단기 기억은 장기 기억으로 변환될 수 있습니다.
- **장기 기억 (Long-term Memory)**:
  가장 큰 용량을 가지며, 정보를 오랜 기간 저장합니다. 장기 기억은 행동을 결정하는 데 필요한 관련 정보를 제공합니다.

### **RecAgent의 메모리 모듈 구성**

**a. 감각 기억 (Sensory Memory)**

- 정의

  에이전트가 경험한 사건을 **관찰(observation)**로 정의합니다. 예:

  - "David Miller enters recommendation system."
  - "David Smith watches a movie called Fast & Furious."

- 처리 과정

  1. 압축 및 정제
     - LLM을 사용하여 관찰을 간결하고 의미 있는 문장으로 압축합니다.
     - 불필요한 내용을 제거하여 공간을 절약하고 효율성을 향상합니다.
  2. 중요도 점수 할당
     - 아이템 관련 정보(예: 감정, 채팅, 메시지 게시)는 더 높은 중요도로 평가됩니다.
     - 중요도 점수와 함께 **타임스탬프**를 추가하여 기억을 정리합니다.
  3. **출력**:
     압축된 관찰 결과는 **(내용, 중요도 점수, 타임스탬프)** 형태로 저장됩니다.

#### **b. 단기 기억 (Short-term Memory)**

- **역할**:
  감각 기억과 장기 기억을 연결하는 중간 모듈입니다.
- 작동 원리
  - 단기 기억은 비슷한 관찰이 반복될 때 강화됩니다.
  - 특정 관찰이 **K번** 강화되면, 장기 기억으로 변환됩니다.
- 구체적 처리
  - 기존 단기 기억에서 새로운 관찰과의 유사도를 계산합니다.
  - 유사도가 임계값 이상일 경우, 기존 기록을 강화합니다.
  - 강화된 관찰은 요약되어 장기 기억에 저장됩니다.

#### **c. 장기 기억 (Long-term Memory)**

- **역할**:
  중요한 정보를 장기간 저장하여 에이전트의 일관된 행동을 지원합니다.

- 특징

  - 기억은 중요도 점수와 타임스탬프에 따라 잊혀질 확률이 설정됩니다.
  - 중요한 기억은 오래된 경우에도 회상될 가능성이 높아집니다.

- 망각 함수

  ![f3](\img\2025\User_Behavior_Simulation_with_Large_Language_Model_based_Agents\f3.PNG)

  ​

  si ri는 정규화 텀 최근 중요도 스코어로 0~1사이 

  ​

#### **메모리 쓰기 (Memory Writing)**

1. **관찰 처리**:
   감각 기억에서 관련 없는 정보를 제거한 뒤, 단기 기억에 저장.
2. **강화 및 장기 기억 변환**:
   단기 기억이 반복 강화되면 장기 기억으로 변환.

#### **b. 메모리 읽기 (Memory Reading)**

- 현재 관찰과 관련된 정보를 **장기 기억**에서 상위 N개의 기록으로 검색.
- **단기 기억**의 모든 정보를 추가하여 최신 및 일반적인 선호도를 모두 반영.

#### **c. 메모리 반영 (Memory Reflection)**

- 장기 기억을 기반으로 높은 수준의 아이디어를 생성.
- 유사한 기억과 통찰을 병합하여 공간 절약 및 중복 제거.



-> 사람의 기억 방식을 메모리형식으로 반영함 ...  이건 시스템 형식으로 구분하는 경우 좋을것으로 보임 

#### **메모리 쓰기 (Memory Writing)**

- 과정

  1. **초기 입력**:
     관찰 데이터를 감각 기억에 입력하여 관련 없는 정보와 중요하지 않은 정보를 제거.

  2. 단기 기억 저장

     처리된 관찰 데이터는 단기 기억에 저장됨.

     - **강화**: 단기 기억에서 비슷한 관찰이 반복되면 해당 기억이 강화됨.
     - **장기 기억 변환**: 특정 관찰이 K번 강화되면, 장기 기억으로 변환되어 저장됨.

------

#### **2. 메모리 읽기 (Memory Reading)**

- **목적**:
  현재 관찰과 관련된 정보를 메모리에서 추출하여 에이전트의 행동을 지원.
- **과정**:
  1. 장기 기억에서 검색
     - 현재 관찰을 쿼리로 사용하여, 관련된 장기 기억 기록 중 상위 NNN개를 검색.
  2. 단기 기억 포함
     - 모든 단기 기억 데이터를 추가하여, 최근 선호도와 일반적인 선호도를 모두 반영.
- **특징**:
  장기 기억(일반 선호도)과 단기 기억(최근 선호도)을 결합하는 전략은 추천 시스템에서 널리 사용되는 방식으로, 에이전트의 신뢰성과 일관성을 강화함.

------

#### **3. 메모리 반영 (Memory Reflection)**

- **목적**:
  특정 관찰 데이터를 기반으로 **높은 수준의 아이디어**를 생성.
- **과정**:
  1. 작동 범위
     - 이 연산은 오직 장기 기억에서만 수행됨.
  2. 유사 기억 통합
     - 유사한 기억과 통찰을 병합하여 메모리 공간을 절약하고 중복 정보를 제거.
- **특징**:
  - 단순히 정보를 저장하는 것을 넘어, 에이전트가 **의미 있는 통찰**을 생성할 수 있도록 지원.
  - 메모리 공간 최적화를 통해 성능을 향상.



# Results

## User behavior simulator with LLM-based agents

![f_1](\img\2025\User_Behavior_Simulation_with_Large_Language_Model_based_Agents\f_1.PNG)

설계된 시뮬레이터의 전체 프레임워크인 RecAgent는 **Figure 1**에서 볼 수 있음

```
그림1 캡션
a. 시뮬레이터의 간략한 실행 과정.
b. 에이전트 프레임워크, 여기에는 프로파일 모듈, 메모리 모듈, 그리고 액션 모듈이 포함됨.
c. 시뮬레이터의 주요 특징들. 다양한 에이전트가 라운드별로 Pareto 분포를 기반으로 행동하며, 각 라운드에서는 소수의 에이전트(파란색으로 표시됨)만이 행동을 취함. 에이전트 속성을 변경하거나 시뮬레이션 프로세스에 참여하여 추천 시스템 또는 다른 에이전트와 상호작용함으로써 시뮬레이션 과정을 능동적으로 변경할 수 있음.
```

각 사용자에 대해, 우리는 사용자의 행동을 시뮬레이션하기 위해 LLM 기반 에이전트를 구축합니다. 이 에이전트는 **프로필 모듈(Profile Module)**, **메모리 모듈(Memory Module)**, **행동 모듈(Action Module)**로 구성됨

- 프로필 모듈

  사용자의 배경을 결정 ID, 이름, 성별, 나이, 성격, 직업, 관심사 등이 포함됨

  **성격(traits)**은 사용자의 성격을 나타내며, 예를 들어 "동정적인(compassionate)", "야망적인(ambitious)", "낙관적인(optimistic)" 등의 특성이 있습니다.

  **관심사(interests)**는 항목 카테고리로 표현되며, 예를 들어 "SF 영화(sci-fi movies)"나 "코미디 비디오(comedy videos)"와 같은 관심사입니다.

- 메모리 모듈

  에이전트가 과거 행동을 기억하고 환경 속에서 동적으로 진화할 수 있도록 합니다.

  인지 신경과학의 인간 기억 메커니즘을 따라 기억을 감각기억과 단기기억, 장기기억 세가지로 설계함

   **감각 기억**: 환경과 직접 상호작용하며, 환경에서의 원시 관찰(raw observations)을 요약하여 더 유용하고 간결한 내용으로 정리합니다.

  **단기 기억**: 감각 기억과 장기 기억을 연결하는 중간 역할을 합니다. 에이전트가 유사한 관찰을 반복적으로 경험하면, 관련 단기 기억이 강화되고 장기 기억으로 변환됩니다.

  **장기 기억**: 비슷한 환경에서 재사용되거나 새로운 관찰로 일반화할 수 있는 중요한 정보를 저장합니다. 에이전트는 장기 기억을 기반으로 스스로 성찰하며, 특정 관찰에서 고수준 및 추상적 정보를 생성할 수 있습니다.

- 행동 모듈

  에이전트 행동을 생성

  추천 시스템에서 에이전트가 수행할 수 있는 네 가지 행동 유형을 정의함

  **검색 행동(Search behaviors)**: 에이전트가 관심 항목을 능동적으로 검색합니다.

  **탐색 행동(Browsing behaviors)**: 에이전트가 시스템으로부터 추천을 수동적으로 수신합니다.

  **클릭 행동(Clicking behaviors)**: 에이전트가 시청하거나 구매하고자 하는 항목을 선택합니다.

  **다음 페이지 행동(Next-page behaviors)**: 에이전트가 현재 추천되거나 검색된 항목에 만족하지 않을 때 더 많은 결과를 보기 위해 트리거됩니다.

  추천 시스템 외 두 가지 사회적 행동을 시뮬레이션함

  **1:1 채팅(One-to-one chatting)**: 두 사용자가 정보를 논의하고 교환하는 행동으로, 예를 들어 Twitter, WeChat과 같은 온라인 채팅이나 커피숍에서의 오프라인 대화가 포함됩니다. 이러한 행동은 사용자가 논의된 항목과 상호작용하거나 기억을 변경하여 이후 행동에 영향을 미칠 수 있습니다.

  **1:다 방송(One-to-many broadcasting)**: 한 사용자가 다른 사용자들에게 정보를 공유하는 행동으로, 예를 들어 소셜 미디어에 의견을 게시하거나 비즈니스 광고를 보내는 것입니다. 이러한 행동은 공유된 정보를 받은 사용자의 기억과 행동에 영향을 미칠 수 있습니다.






뮬레이터는 라운드 단위로 작동합니다. 각 라운드 동안, 에이전트는 자율적으로 행동 여부를 결정

-> 어쩔 수 없이 라운드단위로 하는듯 

현실 세계에서는 사용자가 추천 시스템이나 소셜 미디어에 접근하는 것과 같은 행동을 다양한 활동 수준에서 수행합니다. 이러한 활동 수준은 일반적으로 **롱테일 분포**를 따르며, 이는 소수의 개인이 매우 활발히 활동하는 반면, 대부분의 사용자는 낮은 빈도로 행동한다는 것을 의미



이러한 특성을 시뮬레이터에 반영하기 위해, 우리는 에이전트의 활동 수준을 **Pareto 분포**를 사용해 모델링![f1](\img\2025\User_Behavior_Simulation_with_Large_Language_Model_based_Agents\f1.PNG)

x_min은 최소 활동 수준을 나타내고, α는 분포의 형태를 제어하는 매개변수

시뮬레이터는 특정 추천 알고리즘에 의존하지 않으며, 자유롭게 지정하거나 변경할 수 있음 -> 무슨 의미지?

또한, 실제 인간이 시뮬레이션 과정에 참여할 수 있도록 지원하며, 인간은 에이전트로 역할을 수행하고 추천 시스템 및 다른 에이전트와 상호작용할 수 있음



에이전트에 "최근 본 영화를 다른 사람과 논의하고 싶을 때 어떤 말을 하겠습니까?"과 같이 질문하여 개입할 수 있음 

## Believability of the simulated user behaviors

추천 시스템에서 시뮬레이션된 행동의 신뢰성을 평가하기 위해 다음 과정을 수행

**사용자 데이터 샘플링**
잘 알려진 추천 데이터셋인 **Movielens-1M**에서 20명의 사용자와 그들의 상호작용 데이터를 샘플링합니다.

**평가용 정답 데이터 생성**
각 사용자에 대해 마지막 a개의 아이템을 **평가용 정답 데이터(T_u)**로 분리하고, 나머지 데이터를 사용하여 **에이전트 프로파일**을 초기화합니다.

**추천 리스트 구성**

- 정답 데이터와 b개의 부정적인 아이템(negative items)을 결합하여 총 a+b개의 아이템으로 구성된 추천 리스트를 만듭니다.
- 이 추천 리스트를 에이전트에 제공하여, 에이전트가 a개의 아이템을 선택하게 합니다.

![f2](\img\2025\User_Behavior_Simulation_with_Large_Language_Model_based_Agents\f2.PNG)

U는 모든 유저 u는 특정 사용자 정답 데이터 세트 T_u와 에이전트가 선택한 아이템 세트 S_u를 정의

recall만을 보는걸로 보임.. 왜 이렇게 했을까?



성능은 메트릭 p로 평가되며, p값이 클수록 성능이 더 우수함을 의미

그리고 정답수와 부정 아이템 수를 정의함 

a: 정답 데이터로 선택할 아이템 수.

b: 부정 아이템 수.

![f_2](\img\2025\User_Behavior_Simulation_with_Large_Language_Model_based_Agents\f_2.PNG)

비교 대상은 아래와 같음 

- **Embedding 방법** [26]
- **RecSim** [6]
- **실제 인간의 선택 (Real Human)**
- **우리의 시뮬레이터** (LLM 기반 에이전트).

그림2의 a 구별능력평가

사람과 비슷한 성능이 나왔으나 평균적으로 사람보다 8%낮긴 함 

사용자 평가를 위해 10개를 추출한건 알겠으나.. 모수가 너무 작고 사람평가 기준이 뭘지 궁금 함 



그림2의 b 평가 방법은 n개의 아이템을 참조 행동으로 설정함 (rb는 real humman)



결과 및 분석 (Figure 2b) -  서로 다른 N 값에 따른 추천 행동 평가 (생성 능력 평가).

**초기 프로파일링**:
에이전트를 사용자의 초기 상호작용 데이터로 프로파일링합니다.

**참조 행동 (RB)**:
이후 N개의 아이템을 참조 행동으로 설정합니다.

1. **N = 5**:
   - **RecAgent**의 승률: 45.0%
   - **RecSim**의 승률: 33.3%
   - RecAgent는 **RecSim 대비 더 높은 신뢰성을 보이는 행동 시퀀스**를 생성.
2. **긴 시퀀스 생성**:
   - 행동 시퀀스가 길어질수록, **RecAgent는 RecSim을 약 5% 정도 상회**하는 성능을 유지.

이 결과는 **RecAgent가 일반적으로 더 신뢰할 수 있는 사용자 행동 시퀀스를 생성한다는 것을 입증**



### Chatting and Broadcasting Behaviors. 

사용자 추천, 채팅, 방송 행동을 동시에 포함하는 데이터셋을 찾는 것은 어려움

- 평가 방법

  시뮬레이터에 **20명의 에이전트**를 배치하고, 5, 10, 15 라운드 실행 후의 채팅 및 방송 행동을 관찰합니다.

  세 명의 평가자를 모집하여, 에이전트의 프로필과 이전에 시청한 영화를 기반으로 다음 질문에 답하게 합니다:

  1. 에이전트의 **프로필**에 비추어 채팅 행동이 신뢰할 만한가?
  2. 에이전트의 **이전 행동**에 비추어 채팅 행동이 신뢰할 만한가?
  3. 에이전트의 **프로필**에 비추어 방송 행동이 신뢰할 만한가?
  4. 에이전트의 **이전 행동**에 비추어 방송 행동이 신뢰할 만한가?

  각 질문에 대해 1~5점(점수가 높을수록 더 신뢰할 만함)을 부여하며, 평가자와 에이전트의 점수를 평균하여 결과를 산출합니다.



#### **결과 및 분석 (Figure 2c)**

1. **평균 점수**
   대부분의 결과가 **4점 이상**으로 나타났습니다. 이는 **우리의 시뮬레이터가 신뢰할 만한 채팅 및 방송 행동을 생성할 수 있음을 시사**합니다.
2. **성능 저하**
   - 실행 라운드가 늘어날수록, 즉 **15라운드 이후**, 모든 질문에서 신뢰성 점수가 4점 이하로 떨어졌습니다.
   - 이는 시뮬레이터의 메모리가 많은 내용을 저장하면서, LLM이 유용한 정보를 추출하는 데 어려움을 겪기 때문으로 보입니다.



-> 평가 방식이 신뢰할만 한지 자체가 의심스러움 



### Believability of the Agent Memory Mechanism























# 참고

- 파레토 분포(Pereto distribution)

파레토 분포 는 이탈리아의 토목 기술자 , 경제학자 , 사회학자인 빌프레도 파레토 의 이름을 따서 명명되었으며, 사회 , 품질 관리 , 과학 , 지구 물리학 , 보험 수학 및 기타 여러 유형의 관찰 가능한 현상을 설명하는 데 사용되는 거듭제곱 확률 분포 

-> 위키

사회의 부의 분포를 설명하는 데 적용되었으며 부의 상당 부분 이 인구 의 소수가 소유하고 있다는 추세에 적합




